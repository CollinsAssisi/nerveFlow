{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########IMPORTS############\n",
    "\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import functional_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########INTEGRATOR FRAMEWORK############\n",
    "\n",
    "#1 Constraint Checks\n",
    "\n",
    "def _check_input_types(t, y0): # Ensure input is Correct\n",
    "    if not (y0.dtype.is_floating or y0.dtype.is_complex):\n",
    "        raise TypeError('`y0` must have a floating point or complex floating point dtype')\n",
    "    if not t.dtype.is_floating:\n",
    "        raise TypeError('`t` must have a floating point dtype')\n",
    "        \n",
    "def _assert_increasing(t): # Check Time is Monotonous\n",
    "    assert_increasing = control_flow_ops.Assert(math_ops.reduce_all(t[1:] > t[:-1]), ['`t` must be monotonic increasing'])\n",
    "    return ops.control_dependencies([assert_increasing])\n",
    "\n",
    "#2 Integrator Class\n",
    "\n",
    "class _Integrator():\n",
    "    \n",
    "    def __init__(self,n_):\n",
    "        self.n_ = n_\n",
    "        \n",
    "    def integrate(self, evol_func, y0, time_grid): # iterator\n",
    "        time_delta_grid = time_grid[1:] - time_grid[:-1]\n",
    "        scan_func = self._make_scan_func(evol_func)\n",
    "        y_grid = functional_ops.scan(scan_func, (time_grid[:-1], time_delta_grid),y0)\n",
    "        return array_ops.concat([[y0], y_grid], axis=0)\n",
    "    \n",
    "    def _make_scan_func(self, evol_func): # stepper function\n",
    "        \n",
    "        def scan_func(y, t_dt): \n",
    "            \n",
    "            n_ = self.n_\n",
    "            \n",
    "            t,dt = t_dt\n",
    "            \n",
    "            if n_>0:\n",
    "                \n",
    "                \n",
    "                dy = self._step_func(evol_func, t, dt, y)\n",
    "                dy = math_ops.cast(dy, dtype=y.dtype)\n",
    "                out = y + dy\n",
    "                \n",
    "                ## Operate on non-integral\n",
    "                \n",
    "                ft = y[-n_:]\n",
    "                \n",
    "                l = tf.zeros(tf.shape(ft),dtype=ft.dtype)\n",
    "                l_ = t-ft\n",
    "                \n",
    "                z = tf.less(y[:n_],F_b)\n",
    "                z_ = tf.greater_equal(out[:n_],F_b)\n",
    "                \n",
    "                df = tf.where(tf.logical_and(z,z_),l_,l)\n",
    "                \n",
    "                ft_ = ft+df\n",
    "                \n",
    "                return tf.concat([out[:-n_],ft_],0)\n",
    "\n",
    "            else:\n",
    "                dy = self._step_func(evol_func, t, dt, y)\n",
    "                dy = math_ops.cast(dy, dtype=y.dtype)\n",
    "                return y + dy\n",
    "        \n",
    "        return scan_func\n",
    "\n",
    "    def _step_func(self, evol_func, t, dt, y):\n",
    "        k1 = evol_func(y, t)\n",
    "        half_step = t + dt / 2\n",
    "        dt_cast = math_ops.cast(dt, y.dtype)\n",
    "\n",
    "        k2 = evol_func(y + dt_cast * k1 / 2, half_step)\n",
    "        k3 = evol_func(y + dt_cast * k2 / 2, half_step)\n",
    "        k4 = evol_func(y + dt_cast * k3, t + dt)\n",
    "        return math_ops.add_n([k1, 2 * k2, 2 * k3, k4]) * (dt_cast / 6)\n",
    "\n",
    "#3 Integral Caller\n",
    "\n",
    "def odeint_fixed(func, y0, t, n_):\n",
    "    t = ops.convert_to_tensor(t, preferred_dtype=dtypes.float64, name='t')\n",
    "    y0 = ops.convert_to_tensor(y0, name='y0')\n",
    "    _check_input_types(t, y0)\n",
    "\n",
    "    with _assert_increasing(t):\n",
    "        return _Integrator(n_).integrate(func, y0, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########SIMULATION FRAMEWORK############\n",
    "\n",
    "scale = 10                           # simulation scale\n",
    "\n",
    "n_n = 12 *scale                      # number of neurons\n",
    "\n",
    "p_n = 9 *scale                       # number of PNs\n",
    "l_n = 3 *scale                       # number of LNs\n",
    "\n",
    "t = np.arange(0.0, 300, 0.01)        # duration of simulation\n",
    "\n",
    "C_m  = [1.0]*n_n                     # Capacitance\n",
    "\n",
    "# Common Current Parameters #\n",
    "\n",
    "g_K  = [10.0]*n_n                    # K conductance\n",
    "g_L  = [0.15]*n_n                    # Leak conductance\n",
    "g_KL  = [0.05]*p_n + [0.02]*l_n      # K leak conductance\n",
    "\n",
    "E_K  = [-95.0]*n_n                   # K Potential\n",
    "E_L  = [-55.0]*p_n + [-50.0]*l_n     # Leak Potential\n",
    "E_KL  = [-95.0]*n_n                  # K Leak Potential\n",
    "\n",
    "# Type Specific Current Parameters #\n",
    "\n",
    "## PNs\n",
    "\n",
    "g_Na = [100.0]*p_n                   # Na conductance\n",
    "g_A  = [10.0]*p_n                    # Transient K conductance\n",
    "\n",
    "E_Na = [50.0]*p_n                    # Na Potential\n",
    "E_A  = [-95.0]*p_n                   # Transient K Potential\n",
    "\n",
    "## LNs\n",
    "\n",
    "g_Ca = [3.0]*l_n                     # Ca conductance\n",
    "g_KCa = [0.3]*l_n                    # Ca dependent K conductance\n",
    "\n",
    "E_Ca = [140.0]*l_n                   # Ca Potential\n",
    "E_KCa = [-90]*l_n                    # Ca dependent K Potential\n",
    "\n",
    "A_Ca = 2*(10**(-4))                  # Ca outflow rate\n",
    "Ca0 = 2.4*(10**(-4))                 # Equilibrium Calcium Concentration\n",
    "t_Ca = 150                           # Ca recovery time constant\n",
    "\n",
    "# Synaptic Current Parameters #\n",
    "\n",
    "## Acetylcholine\n",
    "\n",
    "ach_mat = np.zeros((n_n,n_n))        # Ach Synapse Connectivity Matrix\n",
    "\n",
    "ach_mat[p_n:,:p_n] = np.random.choice([0.,1.],size=(l_n,p_n)) # 50% probability of PN -> LN\n",
    "\n",
    "np.fill_diagonal(ach_mat,0.)         # No self connection\n",
    "\n",
    "\n",
    "n_syn_ach = int(np.sum(ach_mat))     # Number of Acetylcholine (Ach) Synapses \n",
    "alp_ach = [10.0]*n_syn_ach           # Alpha for Ach Synapse\n",
    "bet_ach = [0.2]*n_syn_ach            # Beta for Ach Synapse\n",
    "t_max = 0.3                          # Maximum Time for Synapse\n",
    "t_delay = 0                          # Axonal Transmission Delay\n",
    "A = [0.5]*n_n                        # Synaptic Response Strength\n",
    "g_ach = [0.35]*p_n+[0.3]*l_n         # Ach Conductance\n",
    "E_ach = [0.0]*n_n                    # Ach Potential\n",
    "\n",
    "## GABAa (fast GABA)\n",
    "\n",
    "fgaba_mat = np.zeros((n_n,n_n))      # GABAa Synapse Connectivity Matrix\n",
    "\n",
    "fgaba_mat[:,p_n:] = np.random.choice([0.,1.],size=(n_n,l_n)) # 50% probability of LN -> LN/PN\n",
    "\n",
    "np.fill_diagonal(fgaba_mat,0.)       # No self connection\n",
    "\n",
    "n_syn_fgaba = int(np.sum(fgaba_mat)) # Number of GABAa (fGABA) Synapses\n",
    "alp_fgaba = [10.0]*n_syn_fgaba       # Alpha for fGABA Synapse\n",
    "bet_fgaba = [0.16]*n_syn_fgaba       # Beta for fGABA Synapse\n",
    "V0 = [-20.0]*n_n                     # Decay Potential\n",
    "sigma = [1.5]*n_n                    # Decay Time Constant\n",
    "g_fgaba = [0.8]*p_n+[0.8]*l_n        # fGABA Conductance\n",
    "E_fgaba = [-70.0]*n_n                # fGABA Potential\n",
    "\n",
    "\n",
    "# Other Parameters #\n",
    "\n",
    "F_b = [0.0]*n_n                      # Fire potential\n",
    "\n",
    "\n",
    "# Property Dynamics #\n",
    "\n",
    "\n",
    "def K_prop(V):\n",
    "    \n",
    "    T = 22\n",
    "    \n",
    "    phi = 3.0**((T-36.0)/10)\n",
    "    \n",
    "    V_ = V-(-50)\n",
    "    \n",
    "    alpha_n = 0.02*(15.0 - V_)/(tf.exp((15.0 - V_)/5.0) - 1.0)\n",
    "    beta_n = 0.5*tf.exp((10.0 - V_)/40.0)\n",
    "    \n",
    "    t_n = 1.0/((alpha_n+beta_n)*phi)\n",
    "    n_inf = alpha_n/(alpha_n+beta_n)\n",
    "    \n",
    "    return n_inf, t_n\n",
    "\n",
    "\n",
    "def Na_prop(V):\n",
    "    T = 22\n",
    "    \n",
    "    phi = 3.0**((T-36)/10)\n",
    "    \n",
    "    V_ = V-(-50)\n",
    "    \n",
    "    alpha_m = 0.32*(13.0 - V_)/(tf.exp((13.0 - V_)/4.0) - 1.0)\n",
    "    beta_m = 0.28*(V_ - 40.0)/(tf.exp((V_ - 40.0)/5.0) - 1.0)\n",
    "    \n",
    "    alpha_h = 0.128*tf.exp((17.0 - V_)/18.0)\n",
    "    beta_h = 4.0/(tf.exp((40.0 - V_)/5.0) + 1.0)\n",
    "    \n",
    "    t_m = 1.0/((alpha_m+beta_m)*phi)\n",
    "    t_h = 1.0/((alpha_h+beta_h)*phi)\n",
    "    \n",
    "    m_inf = alpha_m/(alpha_m+beta_m)\n",
    "    h_inf = alpha_h/(alpha_h+beta_h)\n",
    "    \n",
    "    return m_inf, t_m, h_inf, t_h\n",
    "\n",
    "\n",
    "def A_prop(V):\n",
    "    T = 36\n",
    "    \n",
    "    phi = 3.0**((T-23.5)/10)\n",
    "    \n",
    "    m_inf = 1/(1+tf.exp(-(V+60.0)/8.5))\n",
    "    h_inf = 1/(1+tf.exp((V+78.0)/6.0))\n",
    "    \n",
    "    tau_m = 1/(tf.exp((V+35.82)/19.69) + tf.exp(-(V+79.69)/12.7) + 0.37) / phi\n",
    "    \n",
    "    t1 = 1/(tf.exp((V+46.05)/5.0) + tf.exp(-(V+238.4)/37.45)) / phi\n",
    "    t2 = (19.0/phi) * tf.ones(tf.shape(V),dtype=V.dtype)\n",
    "    tau_h = tf.where(tf.less(V,-63.0),t1,t2)\n",
    "    \n",
    "    return m_inf, tau_m, h_inf, tau_h\n",
    "\n",
    "\n",
    "def Ca_prop(V):\n",
    "    \n",
    "    m_inf = 1/(1+tf.exp(-(V+20.0)/6.5))\n",
    "    h_inf = 1/(1+tf.exp((V+25.0)/12))\n",
    "    \n",
    "    tau_m = 1.5\n",
    "    tau_h = 0.3*tf.exp((V-40.0)/13.0) + 0.002*tf.exp((60.0-V)/29)\n",
    "    \n",
    "    return m_inf, tau_m, h_inf, tau_h\n",
    "\n",
    "def KCa_prop(Ca):\n",
    "    T = 26\n",
    "    \n",
    "    phi = 2.3**((T-23.0)/10)\n",
    "    \n",
    "    alpha = 0.01*Ca\n",
    "    beta = 0.02\n",
    "    \n",
    "    tau = 1/((alpha+beta)*phi)\n",
    "    \n",
    "    return alpha*tau*phi, tau\n",
    "\n",
    "\n",
    "# NEURONAL CURRENTS\n",
    "\n",
    "# Common Currents #\n",
    "\n",
    "def I_K(V, n):\n",
    "    return g_K  * n**4 * (V - E_K)\n",
    "\n",
    "def I_L(V):\n",
    "    return g_L * (V - E_L)\n",
    "\n",
    "def I_KL(V):\n",
    "    return g_KL * (V - E_KL)\n",
    "\n",
    "# PN Currents #\n",
    "\n",
    "def I_Na(V, m, h):\n",
    "    return g_Na * m**3 * h * (V - E_Na)\n",
    "\n",
    "def I_A(V, m, h):\n",
    "    return g_A * m**4 * h * (V - E_A)\n",
    "\n",
    "# LN Currents #\n",
    "\n",
    "def I_Ca(V, m, h):\n",
    "    return g_Ca * m**2 * h * (V - E_Ca)\n",
    "\n",
    "def I_KCa(V, m):\n",
    "    T = 26\n",
    "    phi = 2.3**((T-23.0)/10)\n",
    "    return g_KCa * m * phi * (V - E_KCa)\n",
    "\n",
    "# SYNAPTIC CURRENTS\n",
    "\n",
    "def I_ach(o,V):\n",
    "    o_ = tf.Variable([0.0]*n_n**2,dtype=tf.float64)\n",
    "    ind = tf.boolean_mask(tf.range(n_n**2),ach_mat.reshape(-1) == 1)\n",
    "    o_ = tf.scatter_update(o_,ind,o)\n",
    "    o_ = tf.transpose(tf.reshape(o_,(n_n,n_n)))\n",
    "    return tf.reduce_sum(g_ach*tf.transpose(o_*(V-E_ach)),1)\n",
    "\n",
    "def I_fgaba(o,V):\n",
    "    o_ = tf.Variable([0.0]*n_n**2,dtype=tf.float64)\n",
    "    ind = tf.boolean_mask(tf.range(n_n**2),fgaba_mat.reshape(-1) == 1)\n",
    "    o_ = tf.scatter_update(o_,ind,o)\n",
    "    o_ = tf.transpose(tf.reshape(o_,(n_n,n_n)))\n",
    "    return tf.reduce_sum(g_fgaba*tf.transpose(o_*(V-E_fgaba)),1)\n",
    "\n",
    "\n",
    "def I_inj_t(t):\n",
    "    return tf.constant(current_input.T,dtype=tf.float64)[tf.to_int32(t*100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIFFERENTIAL EQUATION FORM\n",
    "\n",
    "def dAdt(X, t): # X is the state vector\n",
    "    \n",
    "    # Assign Current Values\n",
    "    \n",
    "    V_p   = X[0   : p_n] \n",
    "    V_l   = X[p_n : n_n]\n",
    "    \n",
    "    n_K   = X[n_n : 2*n_n]\n",
    "    \n",
    "    m_Na  = X[2*n_n : 2*n_n + p_n]\n",
    "    h_Na  = X[2*n_n + p_n : 2*n_n + 2*p_n]\n",
    "\n",
    "    m_A   = X[2*n_n + 2*p_n : 2*n_n + 3*p_n]\n",
    "    h_A   = X[2*n_n + 3*p_n : 2*n_n + 4*p_n]\n",
    "    \n",
    "    m_Ca  = X[2*n_n + 4*p_n : 2*n_n + 4*p_n + l_n]\n",
    "    h_Ca  = X[2*n_n + 4*p_n + l_n: 2*n_n + 4*p_n + 2*l_n]\n",
    "    \n",
    "    m_KCa = X[2*n_n + 4*p_n + 2*l_n : 2*n_n + 4*p_n + 3*l_n]\n",
    "    Ca    = X[2*n_n + 4*p_n + 3*l_n: 2*n_n + 4*p_n + 4*l_n]\n",
    "\n",
    "    o_ach = X[6*n_n : 6*n_n + n_syn_ach]\n",
    "    o_fgaba = X[6*n_n + n_syn_ach : 6*n_n + n_syn_ach + n_syn_fgaba]\n",
    "    \n",
    "    fire_t = X[-n_n:]\n",
    "    \n",
    "    V = X[:n_n]\n",
    "    \n",
    "    \n",
    "    # Evaluate Differentials\n",
    "    \n",
    "    n0,tn = K_prop(V)\n",
    "    \n",
    "    dn_k = - (1.0/tn)*(n_K-n0)\n",
    "    \n",
    "    m0,tm,h0,th = Na_prop(V_p)\n",
    "    \n",
    "    dm_Na = - (1.0/tm)*(m_Na-m0)\n",
    "    dh_Na = - (1.0/th)*(h_Na-h0)\n",
    "    \n",
    "    m0,tm,h0,th = A_prop(V_p)\n",
    "    \n",
    "    dm_A = - (1.0/tm)*(m_A-m0)\n",
    "    dh_A = - (1.0/th)*(h_A-h0)\n",
    "    \n",
    "    m0,tm,h0,th = Ca_prop(V_l)\n",
    "    \n",
    "    dm_Ca = - (1.0/tm)*(m_Ca-m0)\n",
    "    dh_Ca = - (1.0/th)*(h_Ca-h0)\n",
    "    \n",
    "    m0,tm = KCa_prop(Ca)\n",
    "    \n",
    "    dm_KCa = - (1.0/tm)*(m_KCa-m0)\n",
    "    \n",
    "    dCa = - A_Ca*I_Ca(V_l,m_Ca,h_Ca) - (Ca - Ca0)/t_Ca\n",
    "    \n",
    "    CmdV_p = - I_Na(V_p, m_Na, h_Na) - I_A(V_p, m_A, h_A)\n",
    "    CmdV_l = - I_Ca(V_l, m_Ca, h_Ca) - I_KCa(V_l, m_KCa)\n",
    "    \n",
    "    CmdV = tf.concat([CmdV_p,CmdV_l],0)\n",
    "    \n",
    "    dV = (I_inj_t(t) + CmdV - I_K(V, n_K) - I_L(V) - I_KL(V) - I_ach(o_ach,V) - I_fgaba(o_fgaba,V)) / C_m\n",
    "    \n",
    "    A_ = tf.constant(A,dtype=tf.float64)\n",
    "    T_ach = tf.where(tf.logical_and(tf.greater(t,fire_t+t_delay),tf.less(t,fire_t+t_max+t_delay)),A_,tf.zeros(tf.shape(A_),dtype=A_.dtype))\n",
    "    T_ach = tf.multiply(tf.constant(ach_mat,dtype=tf.float64),T_ach)\n",
    "    T_ach = tf.boolean_mask(tf.reshape(T_ach,(-1,)),ach_mat.reshape(-1) == 1)\n",
    "    do_achdt = alp_ach*(1.0-o_ach)*T_ach - bet_ach*o_ach\n",
    "    \n",
    "    T_fgaba = 1.0/(1.0+tf.exp(-(V-V0)/sigma))\n",
    "    T_fgaba = tf.multiply(tf.constant(fgaba_mat,dtype=tf.float64),T_fgaba)\n",
    "    T_fgaba = tf.boolean_mask(tf.reshape(T_fgaba,(-1,)),fgaba_mat.reshape(-1) == 1)\n",
    "    do_fgabadt = alp_fgaba*(1.0-o_fgaba)*T_fgaba - bet_fgaba*o_fgaba\n",
    "    \n",
    "    dfdt = tf.zeros(tf.shape(fire_t),dtype=fire_t.dtype)\n",
    "\n",
    "    out = tf.concat([dV,         dn_k,\n",
    "                     dm_Na,      dh_Na,\n",
    "                     dm_A,       dh_A,\n",
    "                     dm_Ca,      dh_Ca,\n",
    "                     dm_KCa,     \n",
    "                     dCa,        do_achdt,\n",
    "                     do_fgabadt, dfdt   ],0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_input = np.load(\"current.npy\")\n",
    "# current_input = np.genfromtxt(\"current.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Neurons: 120\n",
      "Number of Synapses: 3110\n",
      "120 3110\n"
     ]
    }
   ],
   "source": [
    "#global n_\n",
    "\n",
    "n_ = n_n\n",
    "\n",
    "state_vector =  [-70]* n_n + [0.0]* n_n + [0.0]* (4*p_n) + [0.0]* (3*l_n) + [2.4*(10**(-4))]*l_n + [0]*(n_syn_ach) + [0]*(n_syn_fgaba) + [-500]*n_n\n",
    "state_vector = np.array(state_vector)\n",
    "state_vector = state_vector + 0.01*state_vector*np.random.normal(size=state_vector.shape)\n",
    "\n",
    "print(\"Number of Neurons:\",n_n)\n",
    "print(\"Number of Synapses:\",(n_syn_ach+n_syn_fgaba))\n",
    "print(n_n,(n_syn_ach+n_syn_fgaba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 Running...Session started...Finished in 19.3 secs...Saving...Saved ( Execution Time: 19.531 secs )\n",
      "Batch 2 Running...Session started...Finished in 18.02 secs...Saving...Saved ( Execution Time: 18.308 secs )\n",
      "Batch 3 Running...Session started...Finished in 17.91 secs...Saving...Saved ( Execution Time: 18.161 secs )\n",
      "Batch 4 Running...Session started...Finished in 18.01 secs...Saving...Saved ( Execution Time: 18.303 secs )\n",
      "Batch 5 Running...Session started...Finished in 18.15 secs...Saving...Saved ( Execution Time: 18.424 secs )\n",
      "Batch 6 Running...Session started...Finished in 18.37 secs...Saving...Saved ( Execution Time: 18.738 secs )\n",
      "Batch 7 Running...Session started...Finished in 18.72 secs...Saving...Saved ( Execution Time: 19.136 secs )\n",
      "Batch 8 Running...Session started...Finished in 19.03 secs...Saving...Saved ( Execution Time: 19.448 secs )\n",
      "Batch 9 Running...Session started...Finished in 19.17 secs...Saving...Saved ( Execution Time: 19.71 secs )\n",
      "Batch 10 Running...Session started...Finished in 19.32 secs...Saving...Saved ( Execution Time: 19.845 secs )\n",
      "CPU times: user 17min 42s, sys: 4min 39s, total: 22min 21s\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_batch = 10\n",
    "t_batch = np.array_split(t,n_batch)\n",
    "\n",
    "for n,i in enumerate(t_batch):\n",
    "    \n",
    "    print(\"Batch\",(n+1),\"Running...\",end=\"\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    if n>0:\n",
    "        i = np.append(i[0]-0.01,i)\n",
    "    #print(i.shape)\n",
    "    \n",
    "    init_state = tf.constant(state_vector, dtype=tf.float64)\n",
    "    tensor_state = odeint_fixed(dAdt, init_state, i, n_)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(\"Session started...\",end=\"\")\n",
    "        tf.global_variables_initializer().run()\n",
    "        state = sess.run(tensor_state)\n",
    "        sess.close()\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(\"Finished in\",np.round(t1-t0,2),\"secs...Saving...\",end=\"\")\n",
    "    \n",
    "    state_vector = state[-1,:]\n",
    "    np.save(\"state.batch\"+str(n+1),state)\n",
    "#     np.savetxt(\"state.batch\"+str(n+1),state,delimiter=\",\",fmt='%.3f')\n",
    "    state=None\n",
    "    gc.collect()\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"Saved ( Execution Time:\",np.round(t2-t0,3),\"secs )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
